{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 501,
     "status": "ok",
     "timestamp": 1728038102430,
     "user": {
      "displayName": "35_Suraj Chothe",
      "userId": "13215753797037016176"
     },
     "user_tz": -330
    },
    "id": "MlTbHhMJ-_Xs"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, SimpleRNN, Dense\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "#sample traning text\n",
    "data=\"\"\"\n",
    "Once upon a time in a land far away, there lived a young prince.\n",
    "The prince was brave , strong , and kind . One day, the princes set out\n",
    "on an adventure to discover new lands and find hidden treasures.\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mjuCSYseE7Y6"
   },
   "source": [
    "Data Preparation:\n",
    "The sample text is tokenized using Keras's Tokenizer. Input sequences are created with an increasing number of tokens to predict the next word in each sequence, tokenizer Tokenizer(): Creates an instance of the Tokenizer, which will be used to convert the text into sequences of numbers, tokenizer.fit_on_texts([data]): Fits the tokenizer on the input text (data), creating a dictionary that maps each word to a unique integer, total_words = len(tokenizer.word_index) + 1: tokenizer.word_index is a dictionary that maps words to indices. We add 1 to the total number of words because indices typically start from 1, and we need to account for a padding token (if used)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 480,
     "status": "ok",
     "timestamp": 1728038229794,
     "user": {
      "displayName": "35_Suraj Chothe",
      "userId": "13215753797037016176"
     },
     "user_tz": -330
    },
    "id": "5MwxNWDbFR-9"
   },
   "outputs": [],
   "source": [
    "#preprocess the Text\n",
    "tokenizer=Tokenizer()\n",
    "tokenizer.fit_on_texts([data])\n",
    "total_words=len(tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xELQhb_OGdkN"
   },
   "source": [
    "tokenizer.texts_to_sequences([line]) takes the current sentence (line) and converts it into a list of tokens (or integers). This tokenization step assumes you have a tokenizer object (likely a Tokenizer from Keras or a similar library) that maps words to unique integers. Since texts_to_sequences returns a list of lists (because it processes batches of sentences), the [0] index is used to extract the token list for the current sentence. This for loop iterates over the tokenized sentence, starting from the second token (i=1). In each iteration, it creates an n-gram sequence by taking the first i+1 tokens from the token_list using slicing:token_list[i+1]. This gives a Sequence of increasing length. Each n_gram_sequence is added to the input_sequence list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 505,
     "status": "ok",
     "timestamp": 1728038632793,
     "user": {
      "displayName": "35_Suraj Chothe",
      "userId": "13215753797037016176"
     },
     "user_tz": -330
    },
    "id": "4sBiHsNuGNWo"
   },
   "outputs": [],
   "source": [
    "#convert the text into sequence of tokens\n",
    "input_sequence=[]\n",
    "\n",
    "for line in data.split(\".\"):\n",
    "  token_list = tokenizer.texts_to_sequences([line])[0]\n",
    "  for i in range(1, len(token_list)):\n",
    "    n_gram_sequence=token_list[: i+1]\n",
    "    input_sequence.append(n_gram_sequence)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VDalMC-CIOxP"
   },
   "source": [
    "max sequence len: Finds the length of the longest sequence in input sequences so that all sequences can be padded to the same length, pad sequences (input sequences, maxlen=max_sequence_len, padding='pre\") Pads shorter sequences with zeros at the beginning (pre) so that all sequences are of the same length.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 507,
     "status": "ok",
     "timestamp": 1728039236315,
     "user": {
      "displayName": "35_Suraj Chothe",
      "userId": "13215753797037016176"
     },
     "user_tz": -330
    },
    "id": "Fe_Ix8lqI3r2"
   },
   "outputs": [],
   "source": [
    "#Pad sequences for consistent input size\n",
    "max_sequence_len = max([len(x) for x in input_sequence])\n",
    "input_sequence= np.array(pad_sequences(input_sequence, maxlen=max_sequence_len, padding='pre'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i_mHJhAdJciy"
   },
   "source": [
    "\n",
    "X = input_sequences[::1]: Takes all but the last word of each sequence as the input (features). This is what the model will use to predict the next word. y= input_sequences[:-1]: The last word in the sequence is treated as the label (the word to be predicted). y = np.eye(total_words)[y]: Converts y into a one-hot encoded format, which is needed for classification If total worde is 100 the output will ba a vector of size 100 where only one position (corresepondingg  to the correct word) is 1 and the rest are 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1728039498582,
     "user": {
      "displayName": "35_Suraj Chothe",
      "userId": "13215753797037016176"
     },
     "user_tz": -330
    },
    "id": "7XngcOPYJKbN"
   },
   "outputs": [],
   "source": [
    "#create predictors and labels\n",
    "X, y=input_sequence[:, :-1] , input_sequence[:,-1]\n",
    "y= np.eye(total_words)[y] #one hot encode the labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8ynI5_NZL8s_"
   },
   "source": [
    "#model Architecture\n",
    "An embedding layers to represent words in vectors. A simple RNN layer to learn the sequence of words . A dense layers with softmax activation to predict the next word based on the input sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1365,
     "status": "ok",
     "timestamp": 1728039951263,
     "user": {
      "displayName": "35_Suraj Chothe",
      "userId": "13215753797037016176"
     },
     "user_tz": -330
    },
    "id": "TT9GF-0cLi6X",
    "outputId": "dfd6ae42-cb6b-47b8-8bf6-abc73e4ef674"
   },
   "outputs": [],
   "source": [
    "#bulid the RNN model\n",
    "model=Sequential()\n",
    "model.add(Embedding(total_words, 10, input_length=max_sequence_len-1))\n",
    "model.add(SimpleRNN(150, return_sequences=False))\n",
    "model.add(Dense(total_words, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 225
    },
    "executionInfo": {
     "elapsed": 488,
     "status": "ok",
     "timestamp": 1728039955826,
     "user": {
      "displayName": "35_Suraj Chothe",
      "userId": "13215753797037016176"
     },
     "user_tz": -330
    },
    "id": "EXWlKeKcMtI-",
    "outputId": "b122371d-afc8-4a1c-e068-4daad84637ee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 16, 10)            340       \n",
      "_________________________________________________________________\n",
      "simple_rnn (SimpleRNN)       (None, 150)               24150     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 34)                5134      \n",
      "=================================================================\n",
      "Total params: 29,624\n",
      "Trainable params: 29,624\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#compile the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9615,
     "status": "ok",
     "timestamp": 1728040013442,
     "user": {
      "displayName": "35_Suraj Chothe",
      "userId": "13215753797037016176"
     },
     "user_tz": -330
    },
    "id": "fYkWt19vNSi6",
    "outputId": "8740e7bf-2a5e-44d5-cf58-b62da69aa8a7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sumit\\anaconda3\\envs\\python-cvcourse\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py:108: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "35/35 [==============================] - 0s 11ms/step - loss: 3.6634 - acc: 0.0286\n",
      "Epoch 2/100\n",
      "35/35 [==============================] - 0s 249us/step - loss: 3.4428 - acc: 0.0571\n",
      "Epoch 3/100\n",
      "35/35 [==============================] - 0s 223us/step - loss: 3.3744 - acc: 0.0857\n",
      "Epoch 4/100\n",
      "35/35 [==============================] - 0s 291us/step - loss: 3.3616 - acc: 0.1429\n",
      "Epoch 5/100\n",
      "35/35 [==============================] - 0s 300us/step - loss: 3.2943 - acc: 0.1143\n",
      "Epoch 6/100\n",
      "35/35 [==============================] - 0s 283us/step - loss: 3.1628 - acc: 0.1714\n",
      "Epoch 7/100\n",
      "35/35 [==============================] - 0s 296us/step - loss: 3.0587 - acc: 0.3143\n",
      "Epoch 8/100\n",
      "35/35 [==============================] - 0s 272us/step - loss: 2.9752 - acc: 0.3714\n",
      "Epoch 9/100\n",
      "35/35 [==============================] - 0s 317us/step - loss: 2.8799 - acc: 0.4000\n",
      "Epoch 10/100\n",
      "35/35 [==============================] - 0s 254us/step - loss: 2.7742 - acc: 0.4286\n",
      "Epoch 11/100\n",
      "35/35 [==============================] - 0s 301us/step - loss: 2.6855 - acc: 0.4000\n",
      "Epoch 12/100\n",
      "35/35 [==============================] - 0s 321us/step - loss: 2.6119 - acc: 0.4286\n",
      "Epoch 13/100\n",
      "35/35 [==============================] - 0s 317us/step - loss: 2.5239 - acc: 0.4857\n",
      "Epoch 14/100\n",
      "35/35 [==============================] - 0s 265us/step - loss: 2.4716 - acc: 0.4857\n",
      "Epoch 15/100\n",
      "35/35 [==============================] - 0s 287us/step - loss: 2.4269 - acc: 0.5429\n",
      "Epoch 16/100\n",
      "35/35 [==============================] - 0s 296us/step - loss: 2.3413 - acc: 0.5714\n",
      "Epoch 17/100\n",
      "35/35 [==============================] - 0s 270us/step - loss: 2.2283 - acc: 0.6000\n",
      "Epoch 18/100\n",
      "35/35 [==============================] - 0s 290us/step - loss: 2.1241 - acc: 0.6571\n",
      "Epoch 19/100\n",
      "35/35 [==============================] - 0s 313us/step - loss: 2.0647 - acc: 0.6286\n",
      "Epoch 20/100\n",
      "35/35 [==============================] - 0s 281us/step - loss: 1.9974 - acc: 0.6286\n",
      "Epoch 21/100\n",
      "35/35 [==============================] - 0s 280us/step - loss: 1.8819 - acc: 0.7143\n",
      "Epoch 22/100\n",
      "35/35 [==============================] - 0s 307us/step - loss: 1.8427 - acc: 0.7429\n",
      "Epoch 23/100\n",
      "35/35 [==============================] - 0s 263us/step - loss: 1.7826 - acc: 0.6857\n",
      "Epoch 24/100\n",
      "35/35 [==============================] - 0s 303us/step - loss: 1.7054 - acc: 0.6571\n",
      "Epoch 25/100\n",
      "35/35 [==============================] - 0s 282us/step - loss: 1.6548 - acc: 0.6286\n",
      "Epoch 26/100\n",
      "35/35 [==============================] - 0s 309us/step - loss: 1.5946 - acc: 0.6286\n",
      "Epoch 27/100\n",
      "35/35 [==============================] - 0s 268us/step - loss: 1.5307 - acc: 0.6571\n",
      "Epoch 28/100\n",
      "35/35 [==============================] - 0s 297us/step - loss: 1.4563 - acc: 0.7143\n",
      "Epoch 29/100\n",
      "35/35 [==============================] - 0s 302us/step - loss: 1.4103 - acc: 0.7143\n",
      "Epoch 30/100\n",
      "35/35 [==============================] - 0s 274us/step - loss: 1.3490 - acc: 0.8000\n",
      "Epoch 31/100\n",
      "35/35 [==============================] - 0s 321us/step - loss: 1.2846 - acc: 0.8000\n",
      "Epoch 32/100\n",
      "35/35 [==============================] - 0s 315us/step - loss: 1.2330 - acc: 0.8000\n",
      "Epoch 33/100\n",
      "35/35 [==============================] - 0s 272us/step - loss: 1.2013 - acc: 0.9143\n",
      "Epoch 34/100\n",
      "35/35 [==============================] - 0s 277us/step - loss: 1.1572 - acc: 0.8571\n",
      "Epoch 35/100\n",
      "35/35 [==============================] - 0s 271us/step - loss: 1.0809 - acc: 0.8571\n",
      "Epoch 36/100\n",
      "35/35 [==============================] - 0s 313us/step - loss: 1.0585 - acc: 0.8286\n",
      "Epoch 37/100\n",
      "35/35 [==============================] - 0s 295us/step - loss: 1.0334 - acc: 0.8571\n",
      "Epoch 38/100\n",
      "35/35 [==============================] - 0s 312us/step - loss: 0.9897 - acc: 0.8571\n",
      "Epoch 39/100\n",
      "35/35 [==============================] - 0s 310us/step - loss: 0.9552 - acc: 0.8571\n",
      "Epoch 40/100\n",
      "35/35 [==============================] - 0s 317us/step - loss: 0.9342 - acc: 0.9143\n",
      "Epoch 41/100\n",
      "35/35 [==============================] - 0s 309us/step - loss: 0.9092 - acc: 0.9143\n",
      "Epoch 42/100\n",
      "35/35 [==============================] - 0s 285us/step - loss: 0.8869 - acc: 0.8857\n",
      "Epoch 43/100\n",
      "35/35 [==============================] - 0s 297us/step - loss: 0.8807 - acc: 0.7714\n",
      "Epoch 44/100\n",
      "35/35 [==============================] - 0s 411us/step - loss: 0.8764 - acc: 0.7714\n",
      "Epoch 45/100\n",
      "35/35 [==============================] - 0s 375us/step - loss: 0.8340 - acc: 0.8571\n",
      "Epoch 46/100\n",
      "35/35 [==============================] - 0s 328us/step - loss: 0.8040 - acc: 0.8857\n",
      "Epoch 47/100\n",
      "35/35 [==============================] - 0s 352us/step - loss: 0.7676 - acc: 0.8857\n",
      "Epoch 48/100\n",
      "35/35 [==============================] - 0s 335us/step - loss: 0.7235 - acc: 0.9143\n",
      "Epoch 49/100\n",
      "35/35 [==============================] - 0s 323us/step - loss: 0.7081 - acc: 0.9714\n",
      "Epoch 50/100\n",
      "35/35 [==============================] - 0s 314us/step - loss: 0.7017 - acc: 1.0000\n",
      "Epoch 51/100\n",
      "35/35 [==============================] - 0s 301us/step - loss: 0.6683 - acc: 0.9714\n",
      "Epoch 52/100\n",
      "35/35 [==============================] - 0s 301us/step - loss: 0.6641 - acc: 0.9429\n",
      "Epoch 53/100\n",
      "35/35 [==============================] - 0s 316us/step - loss: 0.6413 - acc: 0.9143\n",
      "Epoch 54/100\n",
      "35/35 [==============================] - 0s 296us/step - loss: 0.6127 - acc: 0.9143\n",
      "Epoch 55/100\n",
      "35/35 [==============================] - 0s 295us/step - loss: 0.5944 - acc: 0.9714\n",
      "Epoch 56/100\n",
      "35/35 [==============================] - 0s 319us/step - loss: 0.5757 - acc: 0.9429\n",
      "Epoch 57/100\n",
      "35/35 [==============================] - 0s 297us/step - loss: 0.5542 - acc: 0.9429\n",
      "Epoch 58/100\n",
      "35/35 [==============================] - 0s 271us/step - loss: 0.5326 - acc: 0.9429\n",
      "Epoch 59/100\n",
      "35/35 [==============================] - 0s 308us/step - loss: 0.5060 - acc: 0.9429\n",
      "Epoch 60/100\n",
      "35/35 [==============================] - 0s 278us/step - loss: 0.4924 - acc: 0.9429\n",
      "Epoch 61/100\n",
      "35/35 [==============================] - 0s 277us/step - loss: 0.4788 - acc: 0.9429\n",
      "Epoch 62/100\n",
      "35/35 [==============================] - 0s 320us/step - loss: 0.4616 - acc: 0.9429\n",
      "Epoch 63/100\n",
      "35/35 [==============================] - 0s 277us/step - loss: 0.4507 - acc: 0.9429\n",
      "Epoch 64/100\n",
      "35/35 [==============================] - 0s 299us/step - loss: 0.4402 - acc: 0.9429\n",
      "Epoch 65/100\n",
      "35/35 [==============================] - 0s 279us/step - loss: 0.4270 - acc: 0.9429\n",
      "Epoch 66/100\n",
      "35/35 [==============================] - 0s 298us/step - loss: 0.4140 - acc: 0.9429\n",
      "Epoch 67/100\n",
      "35/35 [==============================] - 0s 298us/step - loss: 0.4021 - acc: 0.9429\n",
      "Epoch 68/100\n",
      "35/35 [==============================] - 0s 324us/step - loss: 0.3940 - acc: 0.9429\n",
      "Epoch 69/100\n",
      "35/35 [==============================] - 0s 306us/step - loss: 0.3858 - acc: 0.9429\n",
      "Epoch 70/100\n",
      "35/35 [==============================] - 0s 273us/step - loss: 0.3774 - acc: 0.9714\n",
      "Epoch 71/100\n",
      "35/35 [==============================] - 0s 295us/step - loss: 0.3703 - acc: 0.9714\n",
      "Epoch 72/100\n",
      "35/35 [==============================] - 0s 261us/step - loss: 0.3610 - acc: 0.9714\n",
      "Epoch 73/100\n",
      "35/35 [==============================] - 0s 302us/step - loss: 0.3544 - acc: 1.0000\n",
      "Epoch 74/100\n",
      "35/35 [==============================] - 0s 310us/step - loss: 0.3526 - acc: 0.9429\n",
      "Epoch 75/100\n",
      "35/35 [==============================] - 0s 429us/step - loss: 0.3453 - acc: 0.9714\n",
      "Epoch 76/100\n",
      "35/35 [==============================] - 0s 348us/step - loss: 0.3396 - acc: 0.9714\n",
      "Epoch 77/100\n",
      "35/35 [==============================] - 0s 274us/step - loss: 0.3334 - acc: 0.9714\n",
      "Epoch 78/100\n",
      "35/35 [==============================] - 0s 285us/step - loss: 0.3286 - acc: 0.9714\n",
      "Epoch 79/100\n",
      "35/35 [==============================] - 0s 303us/step - loss: 0.3175 - acc: 0.9714\n",
      "Epoch 80/100\n",
      "35/35 [==============================] - 0s 304us/step - loss: 0.3129 - acc: 0.9714\n",
      "Epoch 81/100\n",
      "35/35 [==============================] - 0s 310us/step - loss: 0.3098 - acc: 0.9714\n",
      "Epoch 82/100\n",
      "35/35 [==============================] - 0s 300us/step - loss: 0.3103 - acc: 0.9714\n",
      "Epoch 83/100\n",
      "35/35 [==============================] - 0s 264us/step - loss: 0.3020 - acc: 0.9429\n",
      "Epoch 84/100\n",
      "35/35 [==============================] - 0s 276us/step - loss: 0.2958 - acc: 0.9429\n",
      "Epoch 85/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35/35 [==============================] - 0s 289us/step - loss: 0.2970 - acc: 0.9429\n",
      "Epoch 86/100\n",
      "35/35 [==============================] - 0s 299us/step - loss: 0.2867 - acc: 0.9429\n",
      "Epoch 87/100\n",
      "35/35 [==============================] - 0s 280us/step - loss: 0.2744 - acc: 0.9714\n",
      "Epoch 88/100\n",
      "35/35 [==============================] - 0s 285us/step - loss: 0.2673 - acc: 0.9714\n",
      "Epoch 89/100\n",
      "35/35 [==============================] - 0s 248us/step - loss: 0.2617 - acc: 0.9714\n",
      "Epoch 90/100\n",
      "35/35 [==============================] - 0s 287us/step - loss: 0.2552 - acc: 0.9714\n",
      "Epoch 91/100\n",
      "35/35 [==============================] - 0s 304us/step - loss: 0.2475 - acc: 0.9714\n",
      "Epoch 92/100\n",
      "35/35 [==============================] - 0s 274us/step - loss: 0.2428 - acc: 1.0000\n",
      "Epoch 93/100\n",
      "35/35 [==============================] - 0s 273us/step - loss: 0.2420 - acc: 1.0000\n",
      "Epoch 94/100\n",
      "35/35 [==============================] - 0s 263us/step - loss: 0.2456 - acc: 1.0000\n",
      "Epoch 95/100\n",
      "35/35 [==============================] - 0s 297us/step - loss: 0.2376 - acc: 1.0000\n",
      "Epoch 96/100\n",
      "35/35 [==============================] - 0s 277us/step - loss: 0.2301 - acc: 1.0000\n",
      "Epoch 97/100\n",
      "35/35 [==============================] - 0s 294us/step - loss: 0.2292 - acc: 1.0000\n",
      "Epoch 98/100\n",
      "35/35 [==============================] - 0s 322us/step - loss: 0.2283 - acc: 1.0000\n",
      "Epoch 99/100\n",
      "35/35 [==============================] - 0s 282us/step - loss: 0.2287 - acc: 1.0000\n",
      "Epoch 100/100\n",
      "35/35 [==============================] - 0s 257us/step - loss: 0.2291 - acc: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1f9f536c4e0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train the model\n",
    "model.fit(X,y, epochs=100, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZP9Kx4ANN1eM"
   },
   "source": [
    "predict_next_word: Function to predict the next num_words given some seed_text.\n",
    "tokenizer.texts_to_sequences([seed_text])[0]: Converts the seed text into a sequence of integers. pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre\"): Pads the token list to match the input length required by the model, model.predict(token_list): The model predicts probabilities for each word in the vocabulary. np.argmax(predicted): Retrieves the index of the word with the highest probability, for word, index in tokenizer.word_index.items():: Finds the word corresponding to the predicted index. seed_text +=**+ output word: Appends the predicted word to the seed text. return seed_text: Returns the seed text with predicted words appended. Testing the Model: python Copy code seed_text = \"The prince\" next_words = 5 print(predict_next_word(seed_text, next_words)) seed_text = \"The prince\": The seed text for which you want to generate the next few words. next words = 5: The number of words you want to predict. print(predict_next_word(seed_text, next_words)): Prints the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1728040358813,
     "user": {
      "displayName": "35_Suraj Chothe",
      "userId": "13215753797037016176"
     },
     "user_tz": -330
    },
    "id": "LjFazFPqNeER"
   },
   "outputs": [],
   "source": [
    "#function to predict the next words\n",
    "def predict_next_word(seed_text,num_words):\n",
    "  for _ in range(num_words):\n",
    "    token_list= tokenizer.texts_to_sequences([seed_text])[0]\n",
    "    token_list=pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
    "    predicted=np.argmax(model.predict(token_list), axis=-1)\n",
    "    output_word=\"\"\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "      if index==predicted:\n",
    "        output_word=word\n",
    "        break\n",
    "    seed_text+=\" \"+ output_word\n",
    "\n",
    "  return seed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 537,
     "status": "ok",
     "timestamp": 1728040827048,
     "user": {
      "displayName": "35_Suraj Chothe",
      "userId": "13215753797037016176"
     },
     "user_tz": -330
    },
    "id": "TwSRcro2O1A7",
    "outputId": "090a31df-dbf8-4e62-958d-a62cf25adaf5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Prince was brave strong and kind\n"
     ]
    }
   ],
   "source": [
    "seed_text=\"The Prince\"\n",
    "next_words=5\n",
    "print(predict_next_word(seed_text,next_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9SXOW0UbO32N"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyN9ziRhxzoGvBzBNs9PVcoP",
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
